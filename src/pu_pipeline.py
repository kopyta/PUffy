from rn_methods import RnMethods
from labelling_methods import LabellingMethods
from training import Train
from sklearn.metrics import balanced_accuracy_score
from sklearn.metrics import f1_score
from typing import Tuple
import numpy as np
import pandas as pd
import seaborn as sns
import time
import matplotlib.pyplot as plt 
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split


RN_METHODS = ['spy', 'knn', 'kmeans']

class ProcessPipeline():
    """
    Class representing a pipeline for processing, training and evaluating a logistic regression model.

    This class encapsulates tasks for performing a two-step classification of PU data, using:
        - sar and scar labelling, 
        - spy, knn and kmeans for identification of reliable negatives
        - logistic regression as a classifier
        - balanced accuracy and F1 score as evaluation metrics
    """
    def __init__(self, X: pd.DataFrame, y: np.array, Xtest: pd.DataFrame, ytest: np.array) -> None:
        """
        Initialize the ProcessPipeline class.

        Parameters
        ----------
        X : pd.DataFrame
            Features of the training set.
        y : np.array
            Target variable of the training set.
        Xtest : pd.DataFrame
            Features of the test set.
        ytest : np.array
            Target variable of the test set.
        """

        self.X = X
        self.y = y
        self.Xtest = Xtest
        self.ytest = ytest


    def run_labelling(self, prob: int) -> Tuple[int, int]:

        """
        Run labelling methods to generate new labels for positive instances.

        Parameters
        ----------
        prob : int
            Probability with which positive samples will be selected for the new label.

        Returns
        -------
        Tuple[np.array, np.array]
            Tuple of arrays containing the new labels generated by SCAR and SAR methods.

            - s_scar: np.array
                Array of labels generated by the SCAR method.
            - s_sar: np.array
                Array of labels generated by the SAR method.
        """

        labelling = LabellingMethods(prob)

        s_scar = labelling.scar(self.y)
        s_sar_sigma = labelling.sar_sigmoid(self.X, self.y)
        s_sar_cauchy = labelling.sar_cauchy(self.X, self.y)
        s_lr_sigma = labelling.sar_lr_sigmoid(self.X, self.y)

        return s_scar, s_sar_sigma, s_lr_sigma, s_sar_cauchy
    
    def run_rn_methonds(self, s: np.array, rn_type: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """
        Run reliable negatives identification method based on the specified type.

        Parameters
        ----------
        s : np.array
            Array of labels for positive instances.
        rn_type : str
            Type of method for identifying reliable negatives ('spy', 'knn', 'kmeans').

        Returns
        -------
        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]
            A tuple of DataFrames representing the reliable negatives and partitions.

            - rn: DataFrame
                DataFrame representing reliable negatives.
            - p: DataFrame
                DataFrame representing positive partitions.
            - u: DataFrame
                DataFrame representing unlabeled partitions.
        """

        if rn_type not in RN_METHODS:
            raise NameError("There is no such rn method")
        
        rnmethods = RnMethods()
        training = Train()

        if rn_type == 'spy':
            rn, p, u = rnmethods.spy(self.X, s, percent=15, show_treshhold=True)

        elif rn_type == 'knn':
            rn, p, u = rnmethods.knn(self.X, self.y, s)

        elif rn_type == 'kmeans':
            rn, p, u = rnmethods.kmeans(self.X, s)

        return rn, p, u
    
    def run_training(self, rn: pd.DataFrame, p: pd.DataFrame, u: pd.DataFrame, s: np.array) -> LogisticRegression:
        """
        Train a logistic regression model using the processed data.

        Parameters
        ----------
        rn : pd.DataFrame
            DataFrame representing the reliable negatives.
        p : pd.DataFrame
            DataFrame representing the positive partitions.
        u : pd.DataFrame
            DataFrame representing the unlabeled partitions.
        s : np.array
            Array of labels for positive instances.

        Returns
        -------
        LogisticRegression
            Trained logistic regression model.
        """
        training = Train()
        model = training.iterative_LR(self.X, rn, p, u, s, show_results_flag=False)
    
        return model

    def run_metrics(self, model: LogisticRegression) -> Tuple[float, float]:
        """
        Evaluate the trained model using balanced accuracy and F1 score.

        Parameters
        ----------
        model : LogisticRegression
            Trained logistic regression model.

        Returns
        -------
        Tuple[float, float]
            A tuple of floats representing balanced accuracy and F1 score.

            - b: float
                Balanced accuracy score.
            - f: float
                F1 score.
        """
        ypred = model.predict(self.Xtest)
        balanced_accuracy = balanced_accuracy_score(self.ytest, ypred)
        f_score = f1_score(self.ytest, ypred)

        return balanced_accuracy, f_score

def create_result_df(data_frames: list[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]) -> pd.DataFrame:

    result_df= pd.concat([data_frames[0], data_frames[1], data_frames[2], data_frames[3]], axis=1)
    result_df.columns =pd.MultiIndex.from_tuples([('SCAR', 'Balanced accuracy'), ('SCAR', 'F1 score'),('SCAR', 'time'), 
                                                        ('SAR_SIGMA', 'Balanced accuracy'), ('SAR_SIGMA', 'F1 score'), ('SAR_SIGMA', 'time'),
                                                        ('SAR_LR_SIGMA', 'Balanced accuracy'), ('SAR_LR_SIGMA', 'F1 score'), ('SAR_LR_SIGMA', 'time'),
                                                        ('SAR_CAUCHY', 'Balanced accuracy'), ('SAR_CAUCHY', 'F1 score'), ('SAR_CAUCHY', 'time')])
    return result_df

def oracle_naive_method_result(pu_process: ProcessPipeline, type: str, s_array: np.array) -> list[float, float, float]:


    start_time = time.time()
    model = LogisticRegression(random_state=0)
    if type =="ORACLE":
        model.fit(pu_process.X, pu_process.y)
    elif type =="NAIVE":
        model.fit(pu_process.X, s_array)
    balance_accuracy, f1_score = pu_process.run_metrics(model)
    elapsed_time = time.time() - start_time

    print(f"{type} METHOD: balanced accuracy: {balance_accuracy}, f1 score: {f1_score}, run time: {elapsed_time}")
    return [balance_accuracy, f1_score, elapsed_time] 
    

def rn_method_result(pu_process, rn_type, s_array):
    start_time = time.time()
    rn, p, u = pu_process.run_rn_methonds(s_array, rn_type)

    simple_model = LogisticRegression(max_iter=10000)
    iteration_X = pd.concat((rn.iloc[:, :-1], p.iloc[:, :-1]), axis=0)
    iteration_y = pd.concat((rn['s'], p['s']), axis=0)
    simple_model.fit(iteration_X, iteration_y)
    simple_time = time.time() - start_time
    simple_balance_accuracy, simple_f1_score = pu_process.run_metrics(simple_model)

    model = pu_process.run_training(rn, p, u, s_array)
    iterative_time = time.time() - start_time
    balanced_accuracy, f1_score = pu_process.run_metrics(model)

    return [[simple_balance_accuracy, simple_f1_score, simple_time],[balanced_accuracy, f1_score, iterative_time]]



def launch(X: pd.DataFrame, y: pd.DataFrame, Xtest: pd.DataFrame, ytest: pd.DataFrame, prob: int) -> pd.DataFrame :

    """
    Launch the entire pipeline, including  various labelling, various identification of reliable negatives methods,
    various training methods, and evaluation.

    Parameters
    ----------
    X : pd.DataFrame
        Features of the training set.
    y : pd.DataFrame
        Target variable of the training set.
    Xtest : pd.DataFrame
        Features of the test set.
    ytest : pd.DataFrame
        Target variable of the test set.
    prob : int
        Probability with which positive samples will be selected for the new label.

    Returns
    -------
    pd.DataFrame
        Dataframe containing the results of the entire process.
    """
    dfs=[]
    process = ProcessPipeline(X, y, Xtest, ytest)
    s_scar, s_sar_sigma, s_lr_sigma, s_sar_cauchy  = process.run_labelling(prob)
    oracle_method = oracle_naive_method_result(process,"ORACLE", None)
    for s in [s_scar, s_sar_sigma, s_lr_sigma, s_sar_cauchy]:
        metrics_scores = []
        metrics_scores.append(oracle_method)
        metrics_scores.append(oracle_naive_method_result(process,"NAIVE", s))

        for rn_type in RN_METHODS:
            metrics_scores.extend(rn_method_result(process, rn_type, s))
        df_s = pd.DataFrame(metrics_scores, columns =['Balanced accuracy', 'F1 score', 'time'], 
                            index = ['Oracle methods', 'Naive method', 'Spy + LR', 'Spy + Iterative LR',
                                    'KNN + LR', 'KNN + Iterative LR', 'Kmeans + LR',  'Kmeans + Iterative LR'])
        dfs.append(df_s)
        process.X.drop('s', axis=1, inplace=True)

    result_df = create_result_df(dfs)
    print(result_df)

    return result_df

def count_results(df_X:pd.DataFrame, df_y:pd.DataFrame, test_size: float, test_num: int, prob: list[float], flag = False) -> Tuple:
    """
    Counts and prints results for multiple experiments with different test splits and propensity scores.

    Parameters
    ----------
    df_X : pd.DataFrame
        Features.
    df_y : pd.DataFrame
        Target variable.
    test_size : float
        Size of the test set.
    test_num : int
        Number of experiments.
    prob : list[float]
        List of propensity scores.

    """
    result = [[] for i in range(len(prob))]
    for i in range(test_num):
        print(f"DATA SPLIT INTO TEST SET AND TRAINING SET NUMBER {i+1}")
        X, Xtest, y, ytest = train_test_split(df_X, df_y, test_size = test_size)
        X = X.reset_index(drop=True)
        y = y.reset_index(drop=True)
        for j in range(len(prob)):
            print(f"RESULTS FOR PROPENSITY SCORE {prob[j]}")
            scores = launch(X, y, Xtest, ytest, prob[j])
            result[j].append(scores)
    for i in range(len(result)):
        result[i] = pd.concat(result[i], axis = 0)
        if not flag:
            result[i] = result[i].groupby(result[i].index).agg(['mean'])
        else: 
            result[i] = result[i].groupby(result[i].index).agg(['mean', 'std'])
    return *result,

def count_results_without_splitting_dataset(df_X: pd.DataFrame, df_y: pd.DataFrame, df_Xtest: pd.DataFrame, 
                                            df_ytest: pd.DataFrame, test_num: int, prob: list[float], flag = False) -> Tuple:
    """
    Counts and prints results for multiple experiments without splitting the dataset.

    Parameters
    ----------
    df_X : pd.DataFrame
        Features.
    df_y : pd.DataFrame
        Target variable.
    df_Xtest : pd.DataFrame
        Features for the test set.
    df_ytest : pd.DataFrame
        Target variable for the test set.
    test_num : int
        Number of experiments.
    prob : list[Float]
        List of propensity scores.

    """
    result = [[] for i in range(len(prob))]
    for i in range(test_num):
        print(f"EXPERIMENT NR {i}")
        for j in range(len(prob)):
            print(f"RESULTS FOR PROPENSITY SCORE {prob[j]}")
            scores = launch(df_X, df_y, df_Xtest, df_ytest, prob[j])
            result[j].append(scores)

    for i in range(len(result)):
        result[i] = pd.concat(result[i], axis = 0)
        if not flag:
            result[i] = result[i].groupby(result[i].index).agg(['mean'])
        else: 
            result[i] = result[i].groupby(result[i].index).agg(['mean', 'std'])
    return *result,


def plot_performance(results: list[pd.DataFrame], c_seq: np.ndarray, metrics: str, mechanism: str, dataset: str, iterative = True) -> None:
    """
    Function to plot methods' performance given:
    - performance results list
    - label frequency sequence
    - metrics - possible values: 'Balanced accuracy', 'F1 score'
    - label mechanism - possible values: 'SCAR', 'SAR'
    - dataset name
    """
    results_cp = results.copy()
    if iterative == False:
        for i in range(len(results)):
            results_cp[i] = results[i].drop(index=["Spy + Iterative LR", "KNN + Iterative LR", "Kmeans + Iterative LR"], errors='ignore')
    else:
        for i in range(len(results)):
            results_cp[i] = results[i].drop(index=["Spy + LR", "KNN + LR", "Kmeans + LR"], errors='ignore')

    methods = list(results_cp[0].index)
    method_list = ['knn', 'kmeans', 'naive', 'oracle','spy']
    method_list_mod = method_list.copy()    
    method_list_mod[method_list_mod.index('oracle')]='ORACLE'
    method_list_mod[method_list_mod.index('naive')]='NAIVE'
    method_list_mod[method_list_mod.index('knn')]='KNN'
    method_list_mod[method_list_mod.index('kmeans')]='KMEANS'
    method_list_mod[method_list_mod.index('spy')]='SPY'
    columns_list = method_list_mod.copy()
    columns_list.insert(0, 'c')
    res_mean = pd.DataFrame(columns=columns_list,index=range(len(c_seq)))
    res_sd = pd.DataFrame(columns=columns_list,index=range(len(c_seq)))

    i = 0
    for c in c_seq:
            
        res_mean.iloc[i,0]=c
        res_sd.iloc[i,0]=c
        i = i+1

    for i in range(len(c_seq)):
        for j in range(len(methods)):
            res_mean.iloc[i, j+1] = results_cp[i][mechanism][metrics].loc[methods[j], 'mean']
            res_sd.iloc[i, j+1] = results_cp[i][mechanism][metrics].loc[methods[j], 'std']

    sns.set_style("whitegrid")
    sns.set(font_scale=1.2)
    plt.plot(res_mean.c, res_mean.ORACLE, 'k:', label='ORACLE')
    plt.plot(res_mean.c, res_mean.NAIVE, 'k-', label='NAIVE');plt.fill_between(c_seq, np.array(res_mean.NAIVE - res_sd.NAIVE,dtype='float'), np.array(res_mean.NAIVE +res_sd.NAIVE,dtype='float'), color='k', alpha=0.2)
    plt.plot(res_mean.c, res_mean.KNN, 'b-', label='KNN',marker='o'); plt.fill_between(c_seq, np.array(res_mean.KNN - res_sd.KNN,dtype='float'), np.array(res_mean.KNN +res_sd.KNN,dtype='float'), color='b', alpha=0.2)
    plt.plot(res_mean.c, res_mean.KMEANS, 'g-', label='KMEANS',marker='^');plt.fill_between(c_seq, np.array(res_mean.KMEANS - res_sd.KMEANS,dtype='float'), np.array(res_mean.KMEANS +res_sd.KMEANS,dtype='float'), color='g', alpha=0.2)
    plt.plot(res_mean.c, res_mean.SPY, 'r-', label='SPY',marker='X');plt.fill_between(c_seq, np.array(res_mean.SPY - res_sd.SPY,dtype='float'), np.array(res_mean.SPY +res_sd.SPY,dtype='float'), color='m', alpha=0.2)
    plt.legend()
    plt.title(f"Dataset: {dataset} | Label Mechanism: {mechanism.replace('_', ' ')}", fontsize=15)
    plt.xlabel("Label frequency c")
    plt.ylim(-0.1, 1.1)
    plt.ylabel(metrics)
    plt.show()